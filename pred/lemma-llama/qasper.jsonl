{"pred": "The paper uses the dataset of tweets collected by the researchers themselves. The dataset contains tweets that are labeled as fake news by the researchers. The paper uses this dataset to analyze the characteristics of fake news tweets.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "The paper looked at the following Facebook pages: Google News, CNN, BBC News, New York Times, and the Wall Street Journal.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Answer: The approach presented in this paper compares favorably to other WSD approaches that employ word embeddings. The paper shows that the proposed Gaussian Kernel method outperforms the state-of-the-art methods on the benchmark datasets. The Gaussian Kernel method achieves an F1 score of 79.3% on the SCWS dataset, which is significantly higher than the best performing baseline methods. The paper also shows that the Gaussian Kernel method outperforms the best performing baseline methods on the other benchmark datasets. This indicates that the Gaussian Kernel method is able to better utilize the information provided by word embeddings to disambiguate polysemous words.", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The dataset used is the Amazon review dataset.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "Technology", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "Answer: The authors evaluated the performance of their proposed method on the benchmark datasets. They found that their method achieved an improvement of 0.03-0.08 F1 score over the best performing baseline method on the datasets. This shows that their method is able to effectively utilize the available resources and improve the sentiment classification performance.", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "The attention module is pretrained on the ASR task.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "The authors' model outperforms the state of the art results by a large margin. The model achieves a WER of 0.718, which is significantly lower than the best previous result of 0.920. This shows that their model is able to better capture the emotion information in speech, leading to improved speech emotion recognition performance.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "The paper proposes the use of sentiment context and user profile information as additional features to improve the classification of abusive language.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "Answer: GhostVLAD is a language-independent approach for speaker verification that uses a neural network to classify speech segments into \"ghost\" and \"real\" classes. The network is trained to classify speech segments based on their spectral features, and the \"ghost\" class corresponds to speech segments that are not part of the target speaker. The network is then used to classify speech segments during verification, and the \"ghost\" segments are discarded. This approach aims to improve speaker verification performance by removing irrelevant speech segments.", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "True", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "Answer: The baselines were the following:\n\n- ResNet-10 and ResNet-34 with batch normalization and weight normalization\n- Jasper with ResNet-10 and ResNet-34\n- Jasper with ResNet-10 and ResNet-34 and weight normalization\n- Jasper with ResNet-10 and ResNet-34 and batch normalization\n- Jasper with ResNet-10 and ResNet-34 and weight normalization and batch normalization", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "Answer: The authors propose a new low-resource corpus annotation technique called concept-spotting. This technique involves assigning a concept label to each sentence in the corpus, and then using a neural model to classify sentences based on their concept labels. The authors evaluate this technique on the Amazon Mechanical Turk dataset, and show that it achieves good performance compared to other baselines.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "The paper evaluates the models on the following datasets:\n\n- The Stanford Question Answering Dataset (SQuAD)\n- The NewsQA dataset\n- The CNN/Daily Mail dataset\n- The NYTimes dataset", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "English", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "Answer: The paper does not explicitly state how much data is needed to train the task-specific encoder. However, based on the experiments, it appears that the encoder can be trained with as little as 100 annotations per label. The paper mentions that the encoder is trained on 100 annotations per label for the F1 score experiments. The paper also mentions that the encoder is trained on 200 annotations per label for the F1 score experiments with the additional annotation noise. So, it seems that the encoder can be trained with as little as 100 annotations per label, but more data may be needed for better performance.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "Yes", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "Answer: The datasets are collected from two sources. The first source is the Friends sitcom TV show, which is a dataset of utterances from the characters. The second source is Facebook messenger chats, which is a dataset of conversations between two people. The utterances in the Facebook messenger chats are labeled with emotion tokens.", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "Yes", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "Answer: The authors' ensemble method works by first training a set of base models on the training data. They then select the best performing model based on validation data. The selected model is then used to generate a set of candidate answers for each question. The authors then use a neural network to score each candidate answer based on the question and the context. The top scoring answers are then selected as the final answers. The authors claim that this ensemble method allows them to achieve better performance compared to training a single model.", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The human judgements were assembled by native English and Tamil speakers who evaluated 50 sentences each. The sentences were taken from parallel news articles and movie subtitles. The sentences were evaluated for correctness, any special Tamil features such as suffixes in the pronunciation, and word order.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "The proposed system achieves an accuracy of 92.3% on the DL-DE task and 96.2% on the DL-EC task.", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Answer: The source domain is the training data, which consists of labeled utterances from the Amazon Mechanical Turk dataset. The target domain is the unlabeled data, which consists of utterances from the Yelp and IMDB datasets. The goal is to classify the unlabeled data using the labeled data from the source domain.", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "True", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "True", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "1.5 million tweets", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "Answer: The quality of the data is evaluated empirically by comparing the transcripts to the reference translations. The authors manually inspected a subset of the translations and found that the quality was low. They then used a language model to score the translations and found that the scores were low. This indicates that the data quality is low.", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "Answer: The three regularization terms are:\n\n1. L1 regularization: This term is used to control the number of features that are selected. It is defined as the sum of the absolute values of the coefficients of the features. \n\n2. L2 regularization: This term is used to control the magnitude of the coefficients of the features. It is defined as the sum of the squares of the coefficients.\n\n3. Group Lasso regularization: This term is used to control the correlation between the selected features. It is defined as the sum of the squares of the coefficients within each group of features.\n\nIn summary, the L1 regularization term controls the", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "True", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "Tweets that contain URLs that link to fake news stories", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "The encoder is a sequence-decoder on character sequences.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Answer: The authors compared their proposed NMT model with several baseline models, including:\n- SMT models trained on the same data and evaluated on the same test set\n- NMT models trained on the same data and evaluated on the same test set\n- NMT models trained on different data and evaluated on the same test set\n- NMT models trained on the same data and evaluated on different test sets\n- NMT models trained on different data and evaluated on different test sets\nThe authors also compared their proposed model with the state-of-the-art Chinese-English NMT model from Google.", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The dataset is from Sina Weibo, a popular social media platform in China. The dataset contains both a Weibo user profile and a spammer list, which allows us to build a legitimate user and spammer model. The dataset has been validated by 217 Twitter users and contains 19,226 legitimate users and 22,223 spam users. The dataset has been shown to have high quality based on a microblogging experiment.", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "True", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "Yes", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "Answer: The paper evaluates the generated recipes using the following metrics:\n\n1. BLEU-4: A standard metric for machine translation that measures the similarity between the generated recipe and the reference recipe. A higher BLEU-4 score indicates better recipe quality.\n\n2. Recipe Coherence: A metric that measures the coherence of the generated recipe by checking if the steps are related to each other. A higher score indicates better recipe coherence.\n\n3. Recipe Length: The paper also evaluates the length of the generated recipes. A longer recipe is not necessarily better, but it indicates that the model was able to generate a complete recipe.\n\n4. User Evaluation", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "Answer: The authors propose a new self-attention mechanism called adaptive sparse transformers (AST) that assigns a fixed number of heads to each token. This allows the model to focus on a subset of tokens and improve interpretability. The authors show that AST achieves higher interpretability compared to softmax transformers by analyzing the attention weights and the number of heads used for each token. They find that AST assigns a fixed number of heads to each token, while softmax transformers assign a variable number of heads. This allows AST to focus on a subset of tokens and improve interpretability.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The paper does not provide a direct answer to this question. However, the results presented in Table 2 show that the performance of the Estonian model improved considerably after pre-training on the CoNLL-2002 dataset. The F1 score increased from 71.3% to 78.3%. This suggests that pre-training on a large corpus of unlabeled text can significantly improve the performance of a neural model for the NER task.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "Answer: The paper evaluates the quality of the resulting annotations by comparing them to the gold standard annotations. The results show that the annotations generated by the proposed method are of higher quality compared to the annotations generated by the baseline methods. The annotations generated by the proposed method have a Pearson correlation coefficient of 0.62 compared to 0.55 for the baseline methods. This indicates that the proposed method is able to generate annotations that are more consistent with the gold standard annotations.", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "Answer: The paper mentions that the features are obtained from the text corpus. The features are based on the text corpus and are not based on the speech corpus. The features are based on the text corpus and are not based on the speech corpus.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "Answer: The paper uses the following datasets:\n\n1. The CUB dataset contains utterances from the CUB chatbot. This dataset is used to train the CUB model.\n\n2. The CUB dataset contains utterances from the CUB chatbot. This dataset is used to evaluate the CUB model.\n\n3. The CUB dataset contains utterances from the CUB chatbot. This dataset is used to evaluate the CUB model.\n\n4. The CUB dataset contains utterances from the CUB chatbot. This dataset is used to evaluate the CUB model.\n\n5. The CUB dataset contains utterances from the", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The paper looks at 22,941 users.", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "Answer: The paper uses a hybrid approach of neural and rule-based classifiers. The neural classifier is used to identify the most informative keywords, while the rule-based classifier is used to classify the tweets based on the identified keywords. The hybrid approach aims to improve the performance of the classifiers by leveraging the strengths of both methods.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "No", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "Answer: The authors evaluate their proposed transformer-based sparse adaptor on the tasks of head prediction and head selection. For head prediction, they use the SHOT dataset, which consists of the Shannon entropy and Gini coefficient. For head selection, they use the CEF dataset, which consists of the alpha coefficient.", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The languages that are similar to each other are the Sotho languages.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "Answer: The baselines are the following:\n\n1. Random: This baseline randomly selects one user from the dataset as the moderator. \n\n2. Random + User Info: This baseline randomly selects one user from the dataset as the moderator, and uses user information to calculate the sentiment of the comments. \n\n3. Random + User Info + Comment Info: This baseline randomly selects one user from the dataset as the moderator, uses user information to calculate the sentiment of the comments, and also uses comment information to calculate the sentiment of the comments.\n\n4. Random + User Info + Comment Info + Post Info: This baseline randomly selects one user from the", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "Answer: The authors compare their proposed PRU model with two previous RNN models: PTB and BPTT.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "English and German", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "Answer: The baselines are the following:\n\n1. The first baseline is the ASR model trained on the pre-echo data. This model is trained on the pre-echo data and is used as a comparison to the proposed model.\n\n2. The second baseline is the ASR model trained on the post-echo data. This model is trained on the post-echo data and is used as a comparison to the proposed model.\n\n3. The third baseline is the ASR model trained on the pre-echo data and fine-tuned on the post-echo data. This model is trained on the pre-echo data and then fine", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "Answer: The authors compared their proposed model with the following LSTM models:\n\n1. A baseline model that uses a single-layer LSTM with pre-trained word embeddings. \n\n2. A model that uses a multi-layered LSTM with pre-trained word embeddings.\n\n3. A model that uses a multi-layered LSTM with fine-tuned word embeddings.\n\n4. A model that uses a multi-layered LSTM with fine-tuned word embeddings and a sequence-to-sequence attention mechanism.\n\nThe authors found that their proposed model outperformed all of these baselines on the Shenma speech recognition task. The key improvements were:\n\n1. Using a multi-layered", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The paper uses pre-trained word embeddings from the Stanford NLP group.", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "Answer: The authors use a method called \"extracting the Meaning Extraction Method (MEM)\" to obtain psychological dimensions of people. The method involves analyzing the content of user profiles on a social media platform to identify categories of words that are associated with specific psychological dimensions. The categories of words are then used to generate a distribution of psychological dimensions for each user.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The models are evaluated based on the accuracy of the sentences they generate. The accuracy is measured by the percentage of sentences that match the target sentences.", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "Answer: The paper evaluates the models using the following metrics:\n\n1. Zero-shot Language Transfer Accuracy (ZLTA) - This measures the zero-shot transfer accuracy of the models on the target language. It is calculated as the percentage of sentences that are correctly translated from the source language to the target language without any pre-training on the target language.\n\n2. Cross-lingual Zero-shot Transfer Accuracy (XZTA) - This measures the zero-shot transfer accuracy of the models on the target language when pre-trained on a different source language. It is calculated as the percentage of sentences that are correctly translated from the source language to the target language", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "No", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "Answer: The paper compared their model to the following benchmarks:\n\n1. Affective Text dataset: This dataset contains 6,766 sentences labeled with 5 emotions (anger, fear, sadness, surprise, and joy). The paper evaluated their model on this dataset and found that it outperformed the baselines.\n\n2. ISEAR dataset: This dataset contains 1,000 sentences labeled with 6 emotions (anger, fear, sadness, surprise, joy, and disgust). The paper evaluated their model on this dataset and found that it outperformed the baselines.\n\n3. Twitter dataset: This dataset contains 1,000", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "Answer: The paper mentions the following neural network modules that are included in NeuronBlocks:\n\n1. Block-wise neural network layers: These are neural network layers that operate on blocks of input data. Examples include convolutional neural networks, recurrent neural networks, and graph neural networks.\n\n2. Block-wise neural network modules: These are neural network modules that operate on blocks of input data. Examples include convolutional neural network modules, recurrent neural network modules, and graph neural network modules.\n\n3. Block-wise neural network models: These are neural network models that operate on blocks of input data. Examples include convolutional neural network models, recurrent neural network models", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "Answer: The authors test their method on tasks such as sentiment classification, question classification, and topic classification.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Answer: The authors create labels for the following categories: 11 symptoms (e.g. headache, nausea), 9 medication topics (e.g. medication adherence, medication side effects), 38 diagnosis codes (e.g. ICD-9-CM 401.1, ICD-9-CM 401.9), and 13 procedure codes (e.g. CPT 99283, CPT 99284).", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "Answer: The word subspace can represent the semantic meaning of words. It is a low-dimensional linear subspace that is defined in a high-dimensional word vector space. The word subspace is a compact and scalable representation of words, which can be used to encode the semantic meaning of words.", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "They have a background in computational social science and data science.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Answer: The 12 languages covered are English, German, French, Italian, Spanish, Dutch, Polish, Russian, Chinese, Japanese, Korean, and Arabic.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "The dataset used in this paper is the BIFU dataset.", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "Answer: The authors propose a dual encoder model that combines audio and text sequences in an RNN. The audio encoder is a convolutional neural network that extracts features from the audio signal, while the text encoder is an RNN that processes the text sequence. The audio and text encoders are then combined using a recurrent neural network to predict the emotion label. The model is trained end-to-end using the combined audio and text inputs.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "Answer: The authors use a dataset of Wikipedia articles with English class labels from the class category page. The dataset contains 11,000, 11,000, and 11,000 examples for the classes of cs, en, and physics respectively. The dataset is split into a training set of 9,295 examples, a development set of 500 examples, and a test set of 2,205 examples.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "CNN", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The paper evaluated the performance of emotion detection using the Affective Text dataset. The results showed that the best performing model achieved an F1 score of 0.66 on the validation set and 0.61 on the test set. The paper also evaluated the performance of emotion detection using the ISEAR dataset, and the best performing model achieved an F1 score of 0.71 on the validation set and 0.68 on the test set.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The decoder is an encoder-decoder sequence model.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "26%", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "Authors", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "Answer: The paper evaluates the performance of the proposed PAIR framework using the following metrics:\n\n1. Accuracy: The overall accuracy of the PAIR framework is measured by comparing the predicted labels with the ground truth labels. \n\n2. F1 score: The F1 score is a harmonic mean of precision and recall, which is a widely used metric for evaluating classification performance.\n\n3. Cohen's Kappa: Cohen's Kappa is a statistical measure of inter-rater agreement that accounts for chance agreement. It is used to evaluate the agreement between the PAIR framework and the human supervisor.\n\n4. Classification error: The classification error is the proportion", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "Answer: The paper evaluates the performance of sentence embeddings from various methods, including SGNS, SGNS+BERT, SGNS+BERT+LM, SGNS+BERT+LM+POS, SGNS+BERT+LM+POS+NER, SGNS+BERT+LM+POS+NER+NER, SGNS+BERT+LM+POS+NER+NER+NER, SGNS+BERT+LM+POS+NER+NER+NER+NER, SGNS+BERT+LM+POS+NER+NER+NER+NER+NER, SGNS+BERT+LM+POS+NER+NER+NER", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "Answer: The ML methods aim to identify argument components such as claims, premises, and rebuttals. They do not aim to identify specific argument components like fallacies or ad hominem attacks. The focus is on identifying the basic components of arguments that can be used to analyze and evaluate arguments.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "Relation prediction", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "Answer: The authors used the following datasets:\n\n1. CCG parser dataset\n2. CCG parser dataset with phonological information\n3. CCG parser dataset with phonological information and word error rate annotations\n4. CCG parser dataset with phonological information and word error rate annotations, and a subset of the languages removed", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "Answer: The paper addresses the following cyberbullying topics: \n1. Cyberbullying detection \n2. Cyberbullying types (such as racism, sexism, and cyberstalking) \n3. Cyberbullying language patterns and vocabulary changes \n4. Cyberbullying detection using neural models \n5. Cyberbullying detection using word embeddings \n6. Cyberbullying detection using sentiment analysis \n7. Cyberbullying detection using topic modeling \n8. Cyberbullying detection using neural topic modeling \n9. Cyberbullying detection using neural sequence modeling \n10. Cyberbullying detection using neural sequence-to-sequence models \n11.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Answer: The authors evaluated their proposed Cascade Stacked-LSTM model on the following tasks: natural language inference, sentiment classification, and paraphrase identification. They also evaluated the performance of their model on the tasks of language model prediction and sentiment analysis.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "4", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "Yes", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "Answer: The model is applied to two datasets: the first is a subset of the Wikipedia dataset, and the second is a subset of Reddit dataset. The Wikipedia dataset contains conversations between two people, while the Reddit dataset contains conversations between multiple people. The model is trained on the Wikipedia dataset and evaluated on both datasets.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "True", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "700", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "The paper does not provide a direct answer to this question. However, the results suggest that there is a large disparity in the representation of women and men in speech data. For example, the paper finds that women are underrepresented in speech data compared to their presence in the population, and that the percentage of women in speech data is only 22.57%. This suggests that there is a significant imbalance in the speech data analyzed.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "Answer: The least impactful component is the graph neural network (GNN) component. This is because the GNN component is only used to encode the graph structure of the sentences, which is not as important as the other components that encode the textual content of the sentences. The GNN component is also the only component that is not used in the final prediction.", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "Answer: The data was collected from a crowd-sourcing platform. The participants were asked to transcribe speech utterances from a set of recordings. The recordings were made by a single speaker and contained only one utterance per recording. The participants were paid for their contributions.", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "True", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "Answer: The paper proposes a question answering system based on neural language models and rule-based question matching. The neural language models are used to encode the question and retrieve relevant sentences from the corpus. The rule-based question matching is used to identify the type of question and match it with the corresponding sentences. The paper uses a combination of neural language models and rule-based question matching to achieve the best performance.", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "Answer: The paper explores two embedding techniques: semantic similarity and relation-based embedding. Semantic similarity uses a word co-occurrence matrix to calculate the similarity between words, while relation-based embedding uses a neural network to learn a word co-occurrence matrix.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "The best performing model among the author's submissions is FCLSTM+BERT+LSTM, with a performance of 0.93.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "Answer: The paper uses the following toolkits: BERT, BERTweet, BERTweet-Emotion, BERTweet-Emotion-Plus, BERTweet-Emotion-Plus-Plus, BERTweet-Emotion-Plus-Plus-Plus, BERTweet-Emotion-Plus-Plus-Plus-Plus, BERTweet-Emotion-Plus-Plus-Plus-Plus-Plus, BERTweet-Emotion-Plus-Plus-Plus-Plus-Plus-Plus, BERTweet-Emotion-Plus-Plus-Plus-Plus-Plus-Plus-Plus", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "False", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "Answer: The paper uses a sequence-to-sequence model for painting embedding and a sequence-to-sequence model with a pre-trained language model for language style transfer.", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "The transformer layer outperforms the RNN layer in terms of accuracy.", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Answer: The authors evaluated their model on 20 datasets from the BERT benchmark, 13 datasets from the ASR corpus, and 2 datasets from the topic classification task.", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "Answer: The proposed model, ALOHA, outperforms the baselines on the ALOHA dataset. The average performance gain over the best baseline is 0.03. However, the performance gain is not statistically significant. The performance gain is higher for some characters and lower for others. The performance gain is also dependent on the number of participants in the scene. The proposed model is able to identify the correct character in 95.5% of the scenes with 5 participants, but only 86.5% of the scenes with 20 participants.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "Answer: The authors propose a new context representation for relation classification by concatenating the relation vector and the entity vectors. They show that this new representation outperforms the previous entity-centric and relation-centric representations on the SemEval 2010 dataset.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "14 million words", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "Answer: The dataset contains 4 different types of entities: PER (Person), ORG (Organization), LOC (Location) and MISC (Miscellaneous).", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "Multi30K", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "Answer: The invertibility condition is the requirement that the neural network be able to invert the Gaussian noise. This is necessary to ensure that the Gaussian noise can be removed from the input and the original input can be recovered.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "Answer: The experiments are performed on the SQuAD and NewsQA datasets.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "English", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The dataset used is the Amazon review dataset.", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "The authors evaluated their models on a new dataset of 270K food recipes and user ratings. They found that their personalized models achieved higher performance than baselines, indicating that user preferences can be effectively incorporated into recipe generation.", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "Answer: The paper compares the proposed model to several strong baselines, including CTC, CTC+ASR, CTC+ASR+LSTM, and CTC+ASR+LSTM+CRF. The baselines are compared in terms of their performance on the SIGHAN Bakeoff dataset. The results show that the proposed model outperforms all the baselines, indicating that it is a strong baseline for Chinese character segmentation.", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "Answer: The baselines used for evaluation are:\n\n1. Random: This baseline randomly selects headlines from the dataset. It is used to evaluate the significance of the proposed model.\n\n2. RL-Decoder: This baseline uses reinforcement learning to train a decoder to maximize the RL reward. It is used to evaluate the effectiveness of the proposed RL-based approach.\n\n3. RL-Decoder + RL-Encoder: This baseline uses both reinforcement learning to train the decoder and encoder. It is used to evaluate the effectiveness of the proposed RL-based approach with both encoder and decoder.\n\n4. RL-Decoder + RL-Encoder + RL-Decoder: This baseline", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "Answer: The authors found that the distribution of fake news was not statistically different from the distribution of real news. However, they did find that fake news was more likely to be shared by a smaller number of users, while real news was more likely to be shared by a larger number of users. This suggests that fake news may be more concentrated among a smaller group of users, while real news is more widely shared.", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "Answer: The existing approaches for geographic entity resolution are based on matching features of the locations. The paper proposes a new approach that uses a neural network to learn geographic features from text. The paper also proposes a new metric for evaluating the performance of geographic entity resolution.", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "No", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "Answer: The proposed qualitative annotation schema consists of a set of categories that are used to classify the knowledge required to understand a text passage. The categories include annotation of preposition, conjunction, adverb, adverbial clauses, relative clauses, and other linguistic constructs. The categories are used to identify the specific linguistic knowledge that is required to understand a text passage, and the annotations are used to evaluate the performance of models in predicting that knowledge. The categories are defined in a way that allows for a fine-grained analysis of the linguistic knowledge that is required to understand a text passage.", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "Answer: The baseline model used is the Random Forest model.", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "The results show that the proposed strategies can outperform the baseline in terms of solving the game faster and with fewer steps. However, the performance gain is not as significant as expected. The authors attribute this to the fact that the knowledge graph is not fully utilized and the game state representation is still limited.", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "Small BERT", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Answer: The authors use a SVM model trained on unigrams and POS tags, and a BiLSTM model trained on the same features. The SVM model achieves an F1 score of 0.80, while the BiLSTM model achieves an F1 score of 0.69.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "The languages explored are Bulgarian, Catalan, Czech, Danish, Dutch, English, Finnish, French, German, Hungarian, Italian, Polish, Portuguese, Romanian, Russian, Slovak, Spanish, Swedish and Turkish.", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "The sizes of the two datasets are:\n\n- WikiSmall: 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2,000 sentences, 2", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "Yes", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "The paper proposes a novel method to combine rewards for reinforcement learning. The method uses a neural network to encode sentiment and sentiment-specific words, and then uses a pre-trained language model to calculate the sentiment score of each sentence. The sentiment score is then used to calculate the reward for each sentence. The paper claims that this approach can better capture sentiment and sentiment-specific words, and can produce more accurate sentiment classification.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "Answer: The paper proposes a novel tagging scheme that incorporates a constraint to ensure that the tagging is feasible. The constraint is based on the concept of a \"pun\" word, which is a word that can be used to create a pun by substituting a letter or letters. The constraint is that the tagging must be able to identify a pun word if it occurs in the text. This constraint is incorporated into the tagging scheme by using a neural network that is trained to identify pun words. The neural network is used to classify each word in the text as either a pun word or not, and the tagging is performed based on this classification.", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "Answer: The authors obtained the annotated clinical notes from the i2b2 2014 challenge dataset. The dataset contains de-identified clinical notes from the MIMIC-III database. The dataset was created by querying the clinical notes with a set of clinical terms and extracting relevant sentences from the query results. The query terms were derived from the i2b2 2014 challenge task 1.", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "The dataset is annotated using a pre-existing Twitter dataset that was already labeled with depression-related symptoms.", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "True", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "Answer: The dataset of hashtags is sourced from Twitter. The authors collected tweets from the Twitter API and extracted the hashtags from the tweets. The dataset contains 12,284 tweets and 6,126 unique hashtags.", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "English", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "Yes", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "Answer: The authors demonstrate that their model has two main limitations:\n\n1. The model is not able to handle out-of-vocabulary words. The model is trained on a subset of the dataset and does not generalize well to words that are not in the training set. \n\n2. The model is not able to handle sentences that are very short or very long. The model is optimized for sentences of a certain length and does not perform well on sentences that are much shorter or longer.\n\nIn summary, the model is able to generate high-quality sentences for sentences of a certain length and vocabulary, but it has limitations when it comes to out-of-v", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "Yes", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "Persian and English", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Yes", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "110-hour", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "The paper uses the following clinical datasets:\n\n1. MIMIC-III\n2. MIMIC-IV\n3. eICU\n4. NERD\n5. BERT-CLINC", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "Galatasaray and Fenerbahçe", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "Answer: The authors test their conflict method on two tasks: 1) predicting the outcome of a duplicate question pair and 2) predicting the confidence of a duplicate question pair.", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "Answer: The paper proposes a new method called GRU-RE for named entity recognition (NER) that uses a gated recurrent unit (GRU) to model long-term dependencies and a self-attention mechanism to model local interactions. The method achieves state-of-the-art performance on the CoNLL-2003 and OntoNotes 5.0 datasets for English NER, and outperforms previous methods on the Chinese NER datasets. The improvements in F1 score range from 0.01 to 0.03 for English and from 0.01 to 0.06 for Chinese.", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "The network's baseline features are sentiment, emotion, sarcasm, and personality.", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "Yes", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "Answer: The authors define robustness of a model as the ability to control the unbalanced label error. They use the maximum entropy regularization to control the unbalanced label error. The higher the maximum entropy regularization, the stronger the control of unbalanced label error.", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "Answer: The dataset contains 14 million sentences from 24,000 books.", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "Answer: The highest MRR score achieved by the system was 0.432.", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "The paper mentions several methods to detect biases and unwarranted inferences:\n\n1. Linguistic bias: The authors identify two types of linguistic bias: unwarranted inferences and subjective bias. Unwarranted inferences occur when the annotations are based on the images, and subjective bias occurs when the annotations are based on the user's own beliefs. The authors use a linguistic analysis to identify these biases.\n\n2. Stereotype-driven annotations: The authors identify stereotypes that are used to describe the images. For example, the authors identify the stereotype of \"Asian\" as a person who is good at math. The authors use a stereotype", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "Answer: The baseline models are:\n\n1. Random: A random recipe is generated by randomly selecting a recipe from the training set. \n\n2. BERT: A recipe is generated by randomly selecting a recipe from the training set and then fine-tuning BERT on the selected recipe. \n\n3. BERT + BERT: A recipe is generated by randomly selecting a recipe from the training set, fine-tuning BERT on the selected recipe, and then fine-tuning BERT again on the generated recipe.\n\n4. BERT + BERT + BERT: A recipe is generated by randomly selecting a recipe from the training set,", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "True", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The corpus used for the task is the German language corpus of the Leipzig Semantic Change Detection Challenge.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Answer: The authors compared their proposed model against several baselines:\n\n1. Random Guess: A simple baseline that randomly guesses the relation between the two sentences.\n\n2. SVM: A SVM classifier trained on the features extracted from the sentences.\n\n3. SVM+Tag: The SVM classifier with additional features extracted from the syntactic tags of the sentences.\n\n4. SVM+Tag+LSTM: The SVM classifier with additional features extracted from the syntactic tags and the output of a bi-directional LSTM.\n\n5. SVM+Tag+LSTM+Tree: The SVM classifier with additional features extracted from the syntactic tags, the output of a", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "Spanish", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "Answer: The paper shows that ARAML can improve the performance of the baseline model on the COCO dataset by 1.5% in terms of mAP. This improvement is significant and demonstrates the effectiveness of the proposed adversarial reward training.", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "Answer: The authors experimented with the following summarization algorithms: \n- SVM\n- SVM + SVM\n- SVM + SVM + SVM\n- SVM + SVM + SVM + SVM\n- SVM + SVM + SVM + SVM + SVM\n- SVM + SVM + SVM + SVM + SVM + SVM\n- SVM + SVM + SVM + SVM + SVM + SVM + SVM\n- SVM + SVM + SVM + SVM + SVM + SVM + SVM + SVM\n- SVM + SVM + SVM + SVM + SVM + SVM + SVM + SVM + SVM\n- SVM + SVM + SVM + SVM + SVM + SVM + SVM + SVM", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "Answer: The previous state of the art for this task was the baseline model, which achieved an F1 score of 0.47 on the test set. This baseline model simply predicted the most frequent label for each utterance.", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "Hindi, Tamil, Telugu, Bengali, Marathi, Gujarati, and Kannada", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is not good. The model achieves an F1 score of 0.53 on the Chinese reading comprehension task, which is much lower than the F1 score of 0.88 achieved on the English reading comprehension task. This indicates that the model has difficulty adapting to the target language.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "False", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The subjects were presented with a set of 14 sentences, each of which was presented to 11 participants. The sentences were randomly ordered and each participant was presented with the sentences in a different order.", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "Answer: The paper proposes a new neural network model called MRC-Net for paraphrase identification. The model uses a character-level language model to encode sentences and a query encoder to encode query words. The model is trained using a new training objective that assigns weights to different classes based on the difficulty of distinguishing them. The model achieves state-of-the-art performance on the datasets SST-2 and STS-B. The paper also proposes a new dataset called Ontology Entity Recognition (OER) that consists of sentences from news data. The paper shows that the proposed model achieves better performance on OER compared to existing models.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "Answer: The authors provide several pieces of evidence that their model can detect biases in data annotation and collection:\n\n1. They show that the model can identify tweets that are labeled as racism by the majority of annotators but not by the minority of annotators. This suggests that the model can identify cases where the majority of annotators are biased towards labeling a tweet as racist.\n\n2. They show that the model can identify tweets that are labeled as racism by the majority of annotators but not by the minority of annotators. This suggests that the model can identify cases where the majority of annotators are biased towards labeling a tweet as racist.\n\n3", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "The paper uses the BERT dataset.", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "Answer: The paper uses neural network models for learning abusive language detection. The models include convolutional neural networks (CNNs), recurrent neural networks (RNNs), and hybrid neural networks. The hybrid models combine CNNs and RNNs. The paper also evaluates the performance of the models using different feature sets.", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "A set of parameters", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "Answer: The weights are dynamically adjusted based on the training data. The paper proposes a self-training algorithm that iteratively trains a model on the labeled data and then uses the model to label the unlabeled data. The weights of the unlabeled data are then updated based on the model's predictions. This process is repeated until the model converges.", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "Answer: The paper uses a sequence-to-sequence language model architecture with an encoder-decoder framework. The encoder is a bidirectional language model and the decoder is a unidirectional language model. The paper also experiments with a pre-trained language model as the encoder.", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "False", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "Twitter", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "Answer: The authors demonstrate that their proposed NCEL approach achieves state-of-the-art performance on the benchmark datasets. They show that NCEL outperforms the baselines on the NEL and entity-centric datasets, and achieves comparable performance on the relation-centric datasets. The results indicate that NCEL is effective in modeling complex entity linking problems.", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "The training data was translated using the Affective Text corpus. The sentences were translated using the Affective Text corpus and the sentences were then used to train the models.", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "Answer: The paper proposes a new type of neural network architecture called semicharacter, which is a hybrid of character-level and word-level models. The semicharacter model first converts each input sentence into a sequence of words, and then processes the words at the character-level. This allows the model to take advantage of both word-level and character-level information. The paper argues that this hybrid approach can achieve better performance than purely character-level or word-level models.", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "False", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "The features used are the TF-IDF scores of the keywords and the LDA topic distributions.", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "Answer: The paper argues that masking words in the decoder is helpful because it allows the decoder to focus on the most important positions of the text. This is important because the decoder is constrained by the encoder's representation and needs to generate the target sequence from the encoder's representation. By masking words in the decoder, the decoder can focus on the most important positions of the text and generate the target sequence more efficiently. This is especially important for long sequences where the decoder needs to generate the target sequence from a limited encoder representation.", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "Answer: The authors used a multi-class classification model to classify users into one of 20 categories based on their profile information. They trained a classifier on a subset of users and then evaluated the performance on the remaining users. The classifier was trained using a combination of content-based features and profile features.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The eight NER tasks they evaluated on were:", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "Answer: The paper uses a multi-class classification framework to classify tweets into different political classes. The authors define four classes: liberal, conservative, neutral, and fake. They then train a neural network model to classify tweets into these classes based on their content. The model is trained on a dataset of tweets labeled with their political class. The paper does not explicitly mention how the political bias of different sources is included in the model. However, the model is trained on a dataset that includes tweets from different sources, and the authors report that the model is able to classify tweets from different sources with high accuracy. This suggests that the model is able to", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "3", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "Answer: The ancient Chinese dataset was collected from the Chinese internet. It contains 1.7 million Chinese words from ancient Chinese texts.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "The dataset used in this paper is the Flickr Places dataset.", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "Yes", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Twitter", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The corpus contains 53,190 sentences, 8,275 words, and 1,674,769 tokens.", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Answer: The authors compare their proposed method with two baselines:\n\n1. The first baseline is a simple tagging model that assigns a single tag to each word. This baseline is used to evaluate the error analysis of the output.\n\n2. The second baseline is a neural sequence tagging model that uses a BiLSTM-CRF architecture. This baseline is used to evaluate the performance of the proposed method.\n\nThe authors also compare their method with a state-of-the-art sequence tagging model that uses a BERT encoder and a sequence labeling decoder. However, they find that their proposed method outperforms this baseline as well.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "The paper uses the following datasets:\n\n- BCCDC\n- BCCD\n- BCCD+", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "Answer: The paper mentions that they used traditional linguistic features like POS tags, named entity recognition, and sentiment analysis.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "Answer: The training sets of the new versions of ELMo are much larger than the previous versions. The new versions use a total of 26 billion tokens, while the previous versions used only 1 billion tokens. This is a significant increase in the amount of data used for training. The new versions also use a larger number of languages, which further increases the size of the training data.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "Answer: The paper uses the following metrics to evaluate the performance of the proposed knowledge base open world completion (KBOWC) method:\n\n1. Knowledge coverage: This measures the percentage of facts that are covered by the KBOWC model. A higher knowledge coverage indicates that the model has learned a more complete knowledge base.\n\n2. Knowledge learning: This measures the percentage of facts that are learned by the KBOWC model during training. A higher knowledge learning indicates that the model is able to efficiently learn new knowledge.\n\n3. Knowledge inference: This measures the percentage of facts that can be inferred by the KBOWC model. A higher knowledge", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "True", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "Answer: The paper conducts experiments to evaluate the performance of the proposed sentiment classification model. The experiments involve training and testing the model on different datasets, and evaluating the model's ability to classify sentences based on sentiment. The paper also evaluates the model's performance on different types of sentences, such as sarcastic and non-sarcastic sentences.", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Answer: The Gaussian-masked directional multi-head attention (GM-DMA) is a variant of the standard multi-head attention mechanism that uses Gaussian masks to restrict the attention to a specific direction. The Gaussian mask is used to weight the attention scores based on the distance between the query and key vectors. This allows the model to focus on the relevant information in the sequence and ignore irrelevant information. The Gaussian mask is parameterized by a Gaussian kernel that is applied to the attention scores. The Gaussian kernel has a standard deviation σ that controls the width of the Gaussian mask. The Gaussian mask is applied to each head independently, allowing the model to learn different", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "Answer: The authors achieve the state of the art on the following benchmarks:\n\n1. The SimpleQuestions dataset: The proposed method achieves a significant improvement over the previous best model on this dataset, with an absolute error reduction of 76.76%. \n\n2. The WebQuestions dataset: The proposed method achieves a slight improvement over the previous best model on this dataset, with an absolute error reduction of 0.92%. \n\n3. The SimpleQuestions dataset with the relation prediction task removed: The proposed method achieves a significant improvement over the previous best model on this dataset, with an absolute error reduction of 76.76%. \n\n4.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": "The scores of their system were 0.71 for F1, 0.71 for accuracy, 0.71 for recall, and 0.71 for precision.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "Answer: The hyperparameters that were varied in the experiments on the four tasks were:\n\n1. The number of clusters (k) for the k-means algorithm. The authors used k values of 300, 500, 1000, and 2000 for the four tasks.\n\n2. The number of epochs for the k-means algorithm. The authors used 300, 500, and 1000 epochs for the four tasks.\n\n3. The learning rate for the k-means algorithm. The authors used learning rates of 0.01, 0.001, and 0.0001 for the four tasks", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "Answer: The authors consider the following NLP tasks: sentiment classification, question answering, and web search.", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "Answer: The authors compare their proposed s2-classifier framework to two other models: MLP and SVM. They also compare the performance of their proposed s2-classifier framework on two different datasets: speech/non-speech and emotion classification.", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Answer: The authors evaluate their proposed method on the WSJ dataset and the Penn Treebank dataset.", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "Answer: The authors compare their model to several previous methods, including:\n\n1. BERTQA: A baseline model that uses BERT to encode the question and answer, and then uses a linear classifier to predict the answer.\n2. BERTQA+QA: An improved version of BERTQA that uses a question-aware BERT encoder to encode the question and answer.\n3. BERTQA+QA+QA: An even further improved version of BERTQA+QA that uses a question-aware BERT encoder and a question-aware answer encoder.\n4. BERTQA+QA+QA+QA: An even further improved version", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "Answer: The authors provide a systematic comparison of 3 popular neural network frameworks (PyTorch, MXNet, and TensorFlow) in terms of their performance, scalability, and ease of use. They show that each framework has its own strengths and weaknesses, and that choosing the right framework for a specific task can be a challenging and time-consuming process. This supports their claim that many engineers find it a big overhead to choose from multiple frameworks.", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
