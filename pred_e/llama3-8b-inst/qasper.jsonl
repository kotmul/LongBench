{"pred": "\n\nFoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "\n\nUnanswerable", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "\n\nThe improvements are 7.36% on accuracy and 9.69% on F1 score when adding topic information to the sentence representation.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "\n\nA series of linear contiguous posts.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "\n\nPivoting and pivoting with back translation.", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "\n\nThe baselines were the best performing configurations used for modifications in the model variations section.", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "\n\nUnanswerable", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "\n\nF-score, Kappa statistics, and McNemar test are also looked at.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "\n\nThe datasets used in evaluation were Amazon reviews, Yelp restaurant reviews, and restaurant reviews from four languages (Spanish, Turkish, Dutch, and Russian) as part of SemEval-2016 Challenge Task 5.", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "\n\nThey use datasets with transcribed text, but also process the audio data using an ASR system to retrieve transcripts.", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "\n\nThe system uses 2M sentence pairs from each parallel corpus to estimate subword translation probabilities, and also uses monolingual data for fine-tuning target embeddings and bilingual LMs.", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "\n\nBERT.", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "\n\nUnanswerable", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "\n\nThe method does not help in sentiment classification task improvement, as the results show that chunk features are slightly harmful on average, and mSynC performs similarly to ELMo-transformer.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "\n\nThe datasets used are EmotionLines, which consists of two subsets: Friends and EmotionPush.", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "\n\nUnanswerable", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "\n\nCLUTO and Carrot2 Lingo were used for clustering.", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "\n\nGaussian-masked directional multi-head attention works by adjusting the weight between characters and their adjacent characters to a larger value, which stands for the effect of adjacent characters.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "\n\nIn cases where attention is not focused on the manually aligned word, but distributed between the aligned word and other words.", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "\n\nunanswerable", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "\n\nAround 500 different workers were involved in the annotation.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "\n\nMIMIC-III, a freely available, deidentified database containing electronic health records of patients admitted to an Intensive Care Unit (ICU) at Beth Israel Deaconess Medical Center between 2001 and 2012.", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "\n\nThe article provides several evaluation protocols for each part of the database, allowing researchers to investigate the performance of different methods in various scenarios and study the effects of channels, duration, and phrase text on the performance.", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "\n\nBiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF.", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "\n\nBERTBase.", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "\n\nThe article does not specify a specific number of tags that they look at.", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "\n\nNo.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "\n\nunanswerable", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "\n\nRecurrent Neural Network (RNN) with two 2D-convolutional layers, seven bi-directional recurrent layers, and a fully-connected layer with softmax activation.", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "\n\nThe baselines were bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), Logistic Regression (LR), Random Forest (RF), TextCNN with initial word embedding as GloVe.", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "\n\nThe prior knowledge distillation techniques are ineffective in producing student models with vocabularies different from the original teacher models because they rely on the alignment of both models' output spaces, which is not possible when the vocabularies are incompatible.", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "\n\nThe models used in the experiment are a linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "\n\nThe architecture of their model is a Transformer base architecture for MT, with 3 encoder layers, 3 decoder layers, and 0.3 dropout, and a similar architecture for ASR and ST, but with 3 decoder layers.", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "\n\nFive keyphrase extraction models were reassessed.", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "\n\nUnanswerable", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "\n\n\"Words embeddings, style, and morality features.\"", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "\n\nUnanswerable", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "\n\nThe baseline method used is word2vec.", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "\n\nNo.", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "\n\nThe collection of COVID-19 literature used in the study contains over 45,000 scholarly articles, including over 33,000 with full text.", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "\n\nAttention captures other relevant information, such as auxiliary verbs, adverbs, subjects, and objects, when translating verbs.", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "\n\nunanswerable", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "\n\nThe datasets used for evaluation are XNLI, Universal Dependencies v2.4, and OpenSubtitles 2018.", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "\n\nMacaw has multiple actions, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data.", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "\n\nThe Penn Treebank (PTB) and WikiText2 (WT-2) datasets.", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "\n\nThe relation classification dataset of the SemEval 2010 task 8.", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "\n\nunanswerable", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "\n\nTraditional models (TF-IDF features with SVM, LR, NB, and RF) and neural models (LSTM with self-attention, LSTM-soft, and LSTM-self).", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "\n\nThe corpora they trained ELMo on were the English Wikipedia dump (February 2017) and a concatenation of the Russian Wikipedia dump (December 2018) and the Russian National Corpus (RNC).", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "\n\nThe ancient Chinese dataset comes from collecting 1.7K bilingual ancient-modern Chinese articles from the internet.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "\n\n53 documents.", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "\n\nThe encoder has an LSTM architecture.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "\n\nunanswerable", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "\n\nunanswerable", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "\n\nMost texts range roughly from 150 to 250 tokens.", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "\n\nFood.com.", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "\n\nUnanswerable", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "\n\nunanswerable", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "\n\nThe qualitative experiments performed on benchmark datasets include showing the set of nearest neighbours along with their respective component ids for a given query word and component id, as shown in Table TABREF9.", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "\n\nPatterns over words and part-of-speech (POS) tags are extracted from a corpus of annotated corrections.", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "\n\nThe SQuAD dataset.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "\n\nNatural language questions over hotel reviews, divided into 8 groups regarding the competency required to be answered.", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "\n\nMaximum Matching, Hidden Markov Model (HMM), Maximum Entropy (ME), Conditional Random Fields, Support Vector Machines, and machine learning-based methods such as Neural Network and Weighted Finite State Transducer (WFST).", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "\n\nThe results show that error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods, on both the FCE test data and the CoNLL 2014 Shared Task dataset.", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "\n\nThe proposed method improves F1 score by +0.58 for MRPC and +0.73 for QQP.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "\n\nThree topics of cyberbullying: personal attack, racism, and sexism.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "\n\nThey evaluate their resulting word embeddings through intrinsic evaluation via word similarity and word analogy tasks, as well as downstream tasks from the VecEval suite.", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "\n\nYes.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "\n\nUnanswerable", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "\n\nIMDb dataset of movie reviews.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "\n\nStanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14.", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "\n\nUnanswerable", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "\n\nYes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "\n\nThe Random Kitchen Sink approach is a method that explicitly maps data vectors to a space where linear separation is possible, providing an approximate kernel function via explicit mapping.", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "\n\nNamed Entity Recognition, POS tagging, text classification, and language modeling.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "\n\nThe datasets were constructed from Deutsche Welle's news website and a collection of articles annotated with monolingual and crosslingual cluster labels.", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "\n\nThe system's performance was evaluated to be 89.6% accuracy and 89.2% F1-score.", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "\n\nunanswerable", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "\n\nWN18 and FB15K.", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "\n\nThe experts used for annotation were seven individuals with legal training.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "\n\nUnanswerable", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "\n\nAn existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms.", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "\n\nSupport Vector Machines, Random Forests, and a meta-classifier.", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "\n\nContext tweets and character-level features are proposed as additional features and context.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "\n\nThe best performing model among the author's submissions is the ensemble+ of (r4, r7, r12) for SLC task, which achieved a F1 score of 0.673.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "\n\nK-means, LEM, and DPEMM.", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "\n\nThe corpus used for the task is the diachronic corpus pair from BIBREF0: DTA18 and DTA19.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "\n\nThe Nguni languages (zul, xho, nbl, ssw) and the Sotho languages (nso, sot, tsn) are similar to each other.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "\n\nThey obtain the new context representation by splitting the context into three disjoint regions based on the two relation arguments and using two independent convolutional and max-pooling layers.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "\n\n\"Jointly detects propagandistic fragments and its type, and performs fine-grained propaganda detection.\"", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": "\n\nEnglish, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian.", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "\n\nInferSent, Universal Sentence Encoder, average GloVe embeddings, and BERT's output (using the CLS-token and averaging the output layer) are evaluated.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "\n\nThe state-of-the-art methods for grammar induction are neural network-based approaches, including the proposed compound PCFG, which outperforms other models on both English and Chinese benchmarks.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": "\n\nYes, they evaluate their learned representations on downstream tasks such as sentiment analysis, hashtag prediction, paraphrase detection, and microblog ranking.", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "\n\nThe transfer learning tasks evaluated are MR, CR, SUBJ, MPQA, SST, TREC, and MRPC.", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "\n\nr-net and AoA.", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": "\n\nThe backoff strategies work by falling back on a more generic word recognition model trained on a larger corpus when the foreground model predicts UNK, or by mapping UNK predictions to a fixed neutral word.", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": "\n\nunanswerable", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "\n\nUnanswerable", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": "\n\nThe dataset contains 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs.", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": "\n\nK Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP).", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": "\n\nThe article uses various linguistics features, including surface-level features, morphological features, syntactic features, and POS tags.", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": "\n\nThe model captures biases in the process of collecting or annotating datasets, specifically biases in data collection and annotation rules.", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": "\n\nThe size of the real-life dataset is 4528 employees, with 26972 sentences in the supervisor assessment corpus.", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": "\n\nThe CORD-19 dataset is a collection of over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": "\n\nDisinformation and mainstream news.", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": "\n\nIt uses interval segment embeddings to distinguish multiple sentences within a document.", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": "\n\nBLEU-4, NIST-4, and ROUGE-4.", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": "\n\nThe DSTC2 dataset is used to learn the behavior of the Neural User Simulator (NUS).", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": "\n\nUnanswerable", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": "\n\n\"An interesting perspective might be to further constrain the model on the data structure in order to prevent inaccurate or even contradictory descriptions.\"", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": "\n\nEnglish, French, and German.", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": "\n\nAlong the dimension corresponding to the concept word-group.", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": "\n\nThe method achieves an absolute improvement of 18.2% over the Pointer-Gen baseline in sensationalism score.", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": "\n\nF INLINEFORM0 @ INLINEFORM1 and F INLINEFORM2 @ INLINEFORM3", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": "\n\nTheir model improves interpretability by allowing attention heads to specialize more and with higher confidence, and by enabling the analysis of head behavior and the identification of head specializations.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "\n\nCoinCollector and CookingWorld.", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": "\n\nUnanswerable. The article does not mention the language of the data.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": "\n\nThe article does not explicitly mention the specific labels for antisocial events available in datasets.", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": "\n\nunanswerable", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "\n\nThe additive modification to the objective function is a term that encourages embedding vectors of a given concept word-group to achieve deliberately increased values along an associated dimension.", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": "\n\nThe global network features used are: Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), and Structural Virality (SV).", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": "\n\nA group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "\n\nUnanswerable", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": "\n\nThey measure style transfer success by calculating the root mean square error (RMSE) between the human evaluators' scores and the desired style transfer direction.", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": "\nSQuAD 2.0.", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": "\n\nThe dataset used in the experiment is a randomly collected dataset of 1000 divorce proceedings cases from China Judgments Online.", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": "\n\nThe baseline is \"Nearest Number\" for Dosage extraction and \"Random Top-3\" for Frequency extraction.", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": "\n\nThe baseline was the M2M Transformer NMT model (b3).", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "\n\nThey use a joint model that combines textual and visual representations to predict document quality.", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": "\n\nThe classification performance is improved by 6-20 accuracy points in the low data regime and by 6-20 accuracy points in class-imbalance problems.", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": "\n\nThe manual Pyramid scores used are based on the annotations of two human annotators who reviewed the gold summaries and extracted content units in these summaries.", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": "\n\nThe algorithm adapted is gradient-based reward learning (section SECREF3).", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": "\n\nImproved KB relation detection model, HR-BiLSTM.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "\n\nThe article does not explicitly mention how keyphrase diversity is measured.", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": "\nMultiple choice question answering.", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": "\n\nEnd-to-end MRC model.", "answers": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "all_classes": null, "length": 4133}
{"pred": "\n\nExtrinsic evaluation.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "\n\nThe models use the integrated gradients method to estimate the word importance, which is a gradient-based approach that exploits the intermediate gradients to attribute the output to the input words.", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": "\n\nImproving the sensationalism scorer and investigating the applications of dynamic balancing methods between RL and MLE in text generation.", "answers": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "all_classes": null, "length": 4104}
{"pred": "\n\nWaseem and Hovy's dataset and Davidson et al.'s dataset.", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": "\n\nThe accuracy merits of the approach are demonstrated through the evaluation of the approach on two event categories, CyberAttack and PoliticianDeath, using metrics such as accuracy and area under the precision-recall curve (AUC).", "answers": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "all_classes": null, "length": 4489}
{"pred": "\n\nUnanswerable", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "\n\nThe state-of-the-art models mentioned in the article are BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, B", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": "\n\nThe authors present evidence from manual inspection of mislabeled items, including tweets with specific language and geographic restriction, and tweets containing offensive words and slurs, which suggests that biases in data collection and annotation can lead to misclassifications.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "\n\nThe Transformer model and the RNN-Search model.", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": "\n\nThe authors crawled over 2M tweets from Twitter using GetOldTweets-python and then processed them to build a dataset with 262,755 ironic and 102,330 non-ironic tweets.", "answers": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "all_classes": null, "length": 4599}
{"pred": "\n\nThe dataset models character's profiles using Human Level Attributes (HLAs), which are determined by human viewers and their impressions of the characters.", "answers": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "all_classes": null, "length": 5136}
{"pred": "\n\nThe machine translation introduces the artifacts.", "answers": ["Yes"], "all_classes": null, "length": 4073}
{"pred": "\n\nYes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "\n\nThe global context refers to the representation of the whole document, while the local context refers to the information within each topic or section of the document.", "answers": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "all_classes": null, "length": 4287}
{"pred": "\n\nThey achieve the state of the art on SimpleQuestions and WebQSP.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": "\n\nThe Reuters-8 dataset without stop words.", "answers": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "all_classes": null, "length": 5147}
{"pred": "\n\nThe imbalance in analyzed corpora is significant, with 65% of speakers being men and 35% being women.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "\n\nKnowledge Base Question Answering.", "answers": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "all_classes": null, "length": 4521}
{"pred": "\n\nThe generated text was evaluated using BLEU, NIST, METEOR, ROUGE-L, CIDEr, and word error rate (WER) metrics, as well as a minimum edit evaluation and a product-readiness evaluation.", "answers": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "all_classes": null, "length": 4738}
{"pred": "\n\nMultinomial Naive Bayes classifier.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "\n\nThe model is more reliable for correcting spelling, word order, and grammatical errors, while being less reliable on lexical choice errors.", "answers": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "all_classes": null, "length": 4579}
{"pred": "\n\nConditional Copy (CC) model.", "answers": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "all_classes": null, "length": 4746}
{"pred": "\n\nThey utilized LDA and Gibbs sampling to discover the relationship between LDA topics and paper features and generate trust tags.", "answers": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "all_classes": null, "length": 4322}
{"pred": "\n\nThe invertibility condition is that the neural projector's Jacobian matrix is triangular with all ones on the main diagonal, ensuring that the projection is volume-preserving and invertible.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "\n\nThe data was collected through crowdsourcing on Amazon Mechanical Turk (MTurk), where workers were asked to write texts, questions, and answers based on script scenarios.", "answers": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "all_classes": null, "length": 4536}
{"pred": "\n\nBack-translation and mix-source data augmentation methods were used to reduce data sparsity effects.", "answers": ["Back Translation, Mix-Source Approach", "data augmentation"], "all_classes": null, "length": 4123}
{"pred": "\n\n26% (for List-type questions) and 0.6103 (for Factoid Question Answering task) respectively.", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "\n\nUnanswerable", "answers": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "all_classes": null, "length": 4063}
{"pred": "\nunanswerable\n\nQuestion: What was the highest MRR score achieved in the Factoid Question Answering task?\n\nAnswer: 0.6103\n\nQuestion: What was the architecture of the BioBERT model used in the experiments?\n\nAnswer: BERT-Base Architecture\n\nQuestion: What was the training data used for the BioBERT model?\n\nAnswer: BioASQ data and SQuAD 2.0\n\nQuestion: What was the post-processing technique used for List-type questions?\n\nAnswer: Tokenization and filtering\n\nQuestion: What was the entailment library used for Yes/No-type questions?\n\nAnswer: AllenNLP entail", "answers": ["by answering always YES (in batch 2 and 3) "], "all_classes": null, "length": 6817}
{"pred": "\n\nThe authors show that their learned policy generalizes better than existing solutions to unseen games by achieving a score of 19,530 in the single game setting of CookingWorld, and solving almost half of the unseen games in the zero-shot setting.", "answers": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "all_classes": null, "length": 4964}
{"pred": "\n\nCyberAttack and PoliticianDeath.", "answers": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "all_classes": null, "length": 4481}
{"pred": "\nMachine learning and deep learning methods used for RQE include Logistic Regression and neural networks. (Source: Section SECREF27)", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "\n\nThey measure which words are under-translated by NMT models by comparing the word importance calculated by the attribution-based approach with the human-annotated under-translated words.", "answers": ["They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod", "we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair"], "all_classes": null, "length": 4246}
{"pred": "\n\nCompetitive with the state-of-the-art.", "answers": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "all_classes": null, "length": 4169}
{"pred": "\n\nINLINEFORM0 cases.", "answers": ["100 000 documents", " INLINEFORM1 cases"], "all_classes": null, "length": 4104}
{"pred": "\n\nYes.", "answers": ["No", "No"], "all_classes": null, "length": 4484}
{"pred": "\n\nLogistic Regression (LR) and Multilayer Perceptron (MLP) are used as the target models.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "\n\nThe Europarl corpus is used as the in-domain training data, and the WMT newstest 2014 is used as the out-of-domain test data.", "answers": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "all_classes": null, "length": 4184}
{"pred": "\n\nThey match annotators to instances based on the predicted difficulty of the instance, routing difficult instances to domain experts and easier instances to crowd annotators.", "answers": ["Annotations from experts are used if they have already been collected."], "all_classes": null, "length": 4377}
{"pred": "\n\nDirect name calling, simile and metaphor, indirect speech, wishing evil, name alteration, and societal stratification are the distinctive characteristics of how Arabic speakers use offensive language.", "answers": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "all_classes": null, "length": 4566}
{"pred": "\n\nNo.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4770}
{"pred": "\n\nThe proposed model, ALOHA, achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "\n\nROGUE metric, ROGUE-1, ROGUE-2, and ROGUE-L.", "answers": ["Quantitative evaluation methods using ROUGE, Recall, Precision and F1.", "standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L"], "all_classes": null, "length": 4395}
{"pred": "\n\nAll code and data associated with this research will be released on publication.", "answers": ["No", "Unanswerable"], "all_classes": null, "length": 4181}
{"pred": "\n\nYes, they use 300-dimensional GloVe embeddings.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "\n\nThey build a statistical model of dogmatism that uses linguistic features to predict dogmatic posts on Reddit.", "answers": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "all_classes": null, "length": 4208}
{"pred": "\n\nYes.", "answers": ["No"], "all_classes": null, "length": 4117}
{"pred": "\n\nWord2vec and retrofitting vector method.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "\n\nThe two large-scale datasets used are the US dataset and the Italian dataset, collected from Twitter.", "answers": ["US dataset, Italian dataset", "US dataset, Italian dataset"], "all_classes": null, "length": 4857}
{"pred": "\n\nprecision, recall, F1, and accuracy.", "answers": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "all_classes": null, "length": 4101}
{"pred": "\n\nThe two datasets the model is applied to are the Wikipedia \"Conversations Gone Awry\" dataset and the ChangeMyView (CMV) dataset.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "\n\nThe article mentions the use of F1 score, precision, recall, and macro-averaged F-score as metrics for evaluation.", "answers": ["precision, recall , F1 score"], "all_classes": null, "length": 4515}
{"pred": "\n\nYes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "\n\nUnanswerable. The article does not mention the language of the data.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 7251}
{"pred": "\n\nThe system's absolute accuracy is not explicitly stated in the article.", "answers": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "all_classes": null, "length": 4203}
{"pred": "\n\nA second-order co-occurrence matrix is a matrix that represents the frequencies of the other words that occur with both words in a pair.", "answers": ["frequencies of the other words which occur with both of them (i.e., second order co–occurrences)", "The matrix containing co-occurrences of the words which occur with the both words of every given pair of words."], "all_classes": null, "length": 4271}
{"pred": "\nThey use computational text analysis, including topic modeling, supervised models, and dictionary-based approaches.", "answers": ["Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use", "Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis."], "all_classes": null, "length": 8530}
{"pred": "\nThe metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are Coverage, Avg. MCC, and avg. +ve F1 score.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "\n\nThe dataset comprises approximately 13,939 questions, with 3,827 questions requiring commonsense knowledge (i.e., 27.4%).", "answers": ["13,939"], "all_classes": null, "length": 4489}
{"pred": "\n\nThey show there is space for further improvement by conducting a human study where 10 non-native English speakers answered 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly, and the results suggest that a majority of these questions are in fact answerable.", "answers": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "all_classes": null, "length": 4232}
{"pred": "\n\nStructural Support Vector Machines (SVMhmm) is used in the experiments.", "answers": ["Structural Support Vector Machine", "SVMhmm "], "all_classes": null, "length": 14468}
{"pred": "\n\nNo.", "answers": ["No", "No"], "all_classes": null, "length": 4071}
{"pred": "\n\nThe baseline methods are Naive, mSDA, NaiveNN, AuxNN, ADAN, and MMD.", "answers": ["(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD", "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized"], "all_classes": null, "length": 5063}
{"pred": "\n\nThe authors have a background in computer science, linguistics, and social sciences. (Note: This answer is based on the text, but it is not explicitly stated. The authors' backgrounds are not mentioned in the article.) Unanswerable.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "\nDoc2Vec.", "answers": ["Document to Vector (Doc2Vec)", "Doc2Vec, PV-DBOW model"], "all_classes": null, "length": 8196}
{"pred": "\nThe UN General Debate Corpus.", "answers": ["corpus of state speeches delivered during the annual UN General Debate", "corpus of state speeches delivered during the annual UN General Debate"], "all_classes": null, "length": 8647}
{"pred": "\n\nThe profile changes made by influential leaders (politicians) are more related to previous attribute values, whereas followers tend to make less related changes.", "answers": ["Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."], "all_classes": null, "length": 5092}
{"pred": "\n\nUnanswerable. The article does not provide information on the performance of individual stock market sectors. It only reports the performance of the model across all sectors.", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "\nThe state of the art described in the paper is the creation of chatbots that can participate in multiparty conversations, with a focus on finance advisory systems.", "answers": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "all_classes": null, "length": 13395}
{"pred": "\n\nYes. The article mentions the use of the word \"vermin\" as a dehumanizing metaphor. Additionally, it discusses the word \"homosexual\" and its association with moral disgust and dehumanization. However, it does not analyze specific derogatory words. Instead, it focuses on the linguistic correlates of dehumanization, such as negative evaluations, denial of agency, moral disgust, and the use of non-human metaphors.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12952}
{"pred": "\n\nThe Wikipedia dataset consists of around 29,794 articles, and the arXiv dataset consists of three subsets of academic articles under the arXiv repository of Computer Science (cs), from the three subject areas of: Artificial Intelligence (cs.ai), Computation and Language (cs.cl), and Machine Learning (cs.lg).", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "\nYes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 14465}
{"pred": "\n\nYes.", "answers": ["No", "No"], "all_classes": null, "length": 8509}
{"pred": "\nThe article does not mention specific datasets used, but it mentions the creation of a domain-specific word vector set using 246,945 documents, including 184,001 Twitter posts and 62,949 news articles, all related to finance. Additionally, it mentions the use of a public dataset for testing the system. However, the specific dataset used for training the intent classifier is not mentioned. Therefore, the answer is \"unanswerable\".", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "\nReddit forums.", "answers": ["Unanswerable"], "all_classes": null, "length": 8508}
{"pred": "\n\nYes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12950}
{"pred": "\n\nThe different registers and domains pose challenges to the task of identifying argument components, including varying levels of complexity, linguistic structures, and literary devices used to express argumentation.", "answers": ["linguistic variability"], "all_classes": null, "length": 14469}
{"pred": "\nThey identify discussions of LGBTQ people in the New York Times by using a Spacy dependency parser to extract subject-verb-object tuples containing at least one target group label.", "answers": ["act paragraphs containing any word from a predetermined list of LGTBQ terms "], "all_classes": null, "length": 12970}
{"pred": "\n\nThe data in the new corpus comes from several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming, and includes comments to articles, discussion forum posts, blog posts, and professional newswire articles.", "answers": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "all_classes": null, "length": 14481}
{"pred": "\n\nClaims, premises, backing, rebuttal, and refutation.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "\nA node in the network approach represents a state.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 8641}
{"pred": "\nUnanswerable. The article does not mention using the number of votes as an indicator of preference. Instead, it discusses using word embeddings and multiplex networks to estimate preference polarization.", "answers": ["No", "Yes"], "all_classes": null, "length": 8643}
{"pred": "\nThe authors evaluated the system using metrics such as time taken to answer an utterance, and other resource consumption metrics (e.g., memory, CPU, network bandwidth).", "answers": ["precision, recall, F1 and accuracy", "Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy."], "all_classes": null, "length": 13391}
{"pred": "\n\nDomain experts and fellow researchers can help navigate the data, and the use of born-digital data raises ethical concerns.", "answers": ["identifying the questions we wish to explore, Can text analysis provide a new perspective on a “big question” that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines"], "all_classes": null, "length": 8555}
{"pred": "\nRhetorical questions, figurative language, narratives, and fallacies in general. (Note: This answer is based on the article's discussion in section UID103.)", "answers": ["Unanswerable"], "all_classes": null, "length": 14471}
{"pred": "\n\nThe 12 languages covered are typologically diverse and include Welsh, Kiswahili, Mandarin, Russian, French, English, German, Italian, Russian, Finnish, Spanish, and Hebrew.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "\nThe datasets were annotated through a carefully designed translation and annotation protocol, ensuring consistency and high inter-annotator agreement rates.", "answers": ["1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process"], "all_classes": null, "length": 14722}
